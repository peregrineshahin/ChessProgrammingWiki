---
title: TAAI 2018
---
**[Home](Home "Home") \* [Conferences](Conferences "Conferences") \* TAAI 2018**


**TAAI 2018**,  

the 2018 Conference on Technologies and Applications of Artificial Intelligence, [Taichung](https://en.wikipedia.org/wiki/Taichung), [Taiwan](https://en.wikipedia.org/wiki/Taiwan), November 30 - December 2, 2018.



### Deep Learning Algorithms


* [Hanhua Zhu](index.php?title=Hanhua_Zhu&action=edit&redlink=1 "Hanhua Zhu (page does not exist)"), [Tomoyuki Kaneko](Tomoyuki_Kaneko "Tomoyuki Kaneko") (**2018**). *Comparison of Loss Functions for Training of Deep Neural Networks in Shogi*. TAAI 2018
* [Tianhe Wang](index.php?title=Tianhe_Wang&action=edit&redlink=1 "Tianhe Wang (page does not exist)"), [Tomoyuki Kaneko](Tomoyuki_Kaneko "Tomoyuki Kaneko") (**2018**). *Application of Deep Reinforcement Learning in Werewolf Game Agents*. TAAI 2018
* [Hyunwoo Oh](index.php?title=Hyunwoo_Oh&action=edit&redlink=1 "Hyunwoo Oh (page does not exist)"), [Tomoyuki Kaneko](Tomoyuki_Kaneko "Tomoyuki Kaneko") (**2018**). *Deep Recurrent Q-Network with Truncated History*. TAAI 2018


### Artificial Intelligence Applications


* [Yuuto Kosaka](index.php?title=Yuuto_Kosaka&action=edit&redlink=1 "Yuuto Kosaka (page does not exist)"), [Takeshi Ito](Takeshi_Ito "Takeshi Ito") (**2018**). *Examination of Indicators for Estimating Players’ Strength by using Computer Go*. TAAI 2018


### Artificial Intelligence for Games


<a id="cite-note-2" href="#cite-ref-2">[2]</a>



* [Chu-Hsuan Hsueh](Chu-Hsuan_Hsueh "Chu-Hsuan Hsueh"), [I-Chen Wu](I-Chen_Wu "I-Chen Wu"), [Jr-Chang Chen](Jr-Chang_Chen "Jr-Chang Chen"), [Tsan-sheng Hsu](Tsan-sheng_Hsu "Tsan-sheng Hsu") (**2018**). *AlphaZero for a Non-Deterministic Game*. TAAI 2018


 The [AlphaZero](AlphaZero "AlphaZero") algorithm, developed by DeepMind, achieved superhuman levels of play in the games of chess, [shogi](Shogi "Shogi"), and [Go](Go "Go"), by learning without domain-specific knowledge except game rules. This paper investigates whether the algorithm can also learn theoretical values and optimal plays for non-deterministic games. Since the theoretical values of such games are expected win rates, not a simple win, loss or draw, it is worthy investigating the ability of the AlphaZero algorithm to approximate expected win rates of positions. This paper also studies how the algorithm is influenced by a set of hyper-parameters. The tested non-deterministic game is a reduced and solved version of [Chinese dark chess](Chinese_Dark_Chess "Chinese Dark Chess") (CDC), called 2×4 CDC. The experiments show that the AlphaZero algorithm converges nearly to the theoretical values and the optimal plays in many of the settings of the hyper-parameters. To our knowledge, this is the first research paper that applies the AlphaZero algorithm to non-deterministic games.
* [Shogo Takeuchi](Shogo_Takeuchi "Shogo Takeuchi") (**2018**). *Weighted Majority Voting with a Heterogeneous System in the Game of Shogi*. TAAI 2018


 In this paper, we propose a method of the weighted voting with a heterogeneous system in games and propose to assign the strength of engines and win probabilities of positions to the weights for voting. Assigning the strength as the weight will solve the problem of the weaker engines in the majority voting. Additionally, we introduce a sigmoid function for each engine in order to transform an evaluation value to a win probability. By this sigmoid functions, we can compare the win probabilities between the different engines and resolve the problem in the optimistic voting with a heterogeneous system. We performed the tournaments among the proposed system and the other voting systems against a single engine to compare the strength of the voting systems in [shogi](Shogi "Shogi"). From the experimental results, we showed that the proposed method defeats all other voting systems.
* [Taichi Nakayashiki](index.php?title=Taichi_Nakayashiki&action=edit&redlink=1 "Taichi Nakayashiki (page does not exist)"), [Tomoyuki Kaneko](Tomoyuki_Kaneko "Tomoyuki Kaneko") (**2018**). *Learning of Evaluation Functions via Self-Play Enhanced by Checkmate Search*. TAAI 2018


 As shown in [AlphaGo](index.php?title=AlphaGo&action=edit&redlink=1 "AlphaGo (page does not exist)") and [AlphaZero](AlphaZero "AlphaZero"), [reinforcement learning](Reinforcement_Learning "Reinforcement Learning") is effective in learning of evaluation functions (or value networks) in [Go](Go "Go"), [Chess](Chess "Chess") and [Shogi](Shogi "Shogi"). In their training, two procedures are repeated in parallel; self-play with a current evaluation function and improvement of the evaluation function by using game records yielded by recent self-play. Although AlphaGo Zero and AlphaZero have achieved super human performance, it requires enormous computation resources. To alleviate the problem, this paper proposes to incorporate a checkmate solver in self-play. We show that this small enhancement dramatically improves the efficiency in our experiments in Minishogi, via the quality of game records in self-play. It should be noted that our method is still free from human knowledge about a target domain, though the implementation of checkmate solvers is domain dependent
* [Yusaku Mandai](index.php?title=Yusaku_Mandai&action=edit&redlink=1 "Yusaku Mandai (page does not exist)"), [Tomoyuki Kaneko](Tomoyuki_Kaneko "Tomoyuki Kaneko") (**2018**). *Alternative Multitask Training for Evaluation Functions in Game of Go*. TAAI 2018


 For the game of [Go](Go "Go"), [Chess](Chess "Chess"), and [Shogi](Shogi "Shogi") (Japanese Chess), [deep neural networks](Deep_Learning "Deep Learning") (DNNs) have contributed to build accurate evaluation functions and many studies have attempted to create so called the value network which predicts a reward of a given state. A recent study of the value network for the game of Go has shown that a two-headed neural network with two different objectives can be trained effectively and performs better than a single-headed network. One of the two heads is called a value head and the other, policy head, predicts next moves at a given state. This multitask training makes the network more robust and improves the generalization performance. In this paper we show that a simple discriminator network is an alternative target of the multitask learning. Compared to the existing deep neural network, our proposed network can be designed more easily because of its simple output. Experimental results showed that our discriminative target also makes the learning stable and the evaluation function trained by our method is comparable to the training of existing studies in terms of predicting next moves and playing strength.
* [Kiminori Matsuzaki](index.php?title=Kiminori_Matsuzaki&action=edit&redlink=1 "Kiminori Matsuzaki (page does not exist)"), [Madoka Teramura](index.php?title=Madoka_Teramura&action=edit&redlink=1 "Madoka Teramura (page does not exist)") (**2018**). *Interpreting Neural-Network Players for Game 2048*. TAAI 2018


 Game 2048 is a stochastic single-player game and development of strong computer players for 2048 has been based on N-tuple networks trained by reinforcement learning. In our previous study, we developed computer players for game 2048 based on convolutional neural networks (CNNs), and showed by experiments that networks with three or more convolution layers performed much better than 2-convolution network. In this study, we analyze the inner working of our CNNs (i.e. white box approach) to identify the reasons of the performance. Our analysis includes visualization of filters in the first layers and backward trace of the networks for some specific game positions. We had several findings on inner working of our CNNs for game 2048.
* [Kiminori Matsuzaki](index.php?title=Kiminori_Matsuzaki&action=edit&redlink=1 "Kiminori Matsuzaki (page does not exist)") (**2018**). *Empirical Analysis of PUCT Algorithm with Evaluation Functions of Different Quality*. TAAI 2018


 [Monte-Carlo tree search](Monte-Carlo_Tree_Search "Monte-Carlo Tree Search") (MCTS) algorithms play an important role in developing computer players for many games. The performance of MCTS players is often leveraged in combination with offline knowledge, i.e., evaluation functions. In particular, recently [AlphaGo](index.php?title=AlphaGo&action=edit&redlink=1 "AlphaGo (page does not exist)") and [AlphaGo Zero](index.php?title=AlphaGo_Zero&action=edit&redlink=1 "AlphaGo Zero (page does not exist)") achieved a big success in developing strong computer [Go](Go "Go") player by combining evaluation functions consisting of [deep neural networks](Deep_Learning "Deep Learning") with a variant of [PUCT](Christopher_D._Rosin#PUCT "Christopher D. Rosin") (Predictor + [UCB applied to trees](UCT "UCT")). The effect of evaluation functions on the strength of MCTS algorithms, however, has not been investigated well, especially in terms of the quality of evaluation functions. In this study, we address this issue and empirically analyze the AlphaGo's PUCT algorithm by using [Othello](Othello "Othello") (Reversi) as the target game. We investigate the strength of PUCT players using variants of an existing evaluation function of a champion-level computer player. From intensive experiments, we found that the PUCT algorithm works very well especially with a good evaluation function and that the value function has more importance than the policy function in the PUCT algorithm.
## External Links


* [TAAI 2018 – The 2018 Conference on Technologies and Applications of Artificial Intelligence](http://taai2018.asia.edu.tw/)
* [dblp: TAAI 2018 : Taichung, Taiwan](https://dblp.uni-trier.de/db/conf/taai/taai2018.html)


## References


1. <a id="cite-ref-1" href="#cite-note-1">[1]</a> [Technical Program](http://taai2018.asia.edu.tw/wp-content/uploads/2018/11/TAAI2018-SessionsInternationalTrack1128.pdf) (pdf)
2. <a id="cite-ref-2" href="#cite-note-2">[2]</a> Abstracts from [TAAI 2018 Session 2.4 Artificial Intelligence for Games](http://taai2018.asia.edu.tw/wp-content/uploads/2018/11/Session-2.4-Artificial-Intelligence-for-Games.pdf) (pdf)

**[Up one Level](Conferences "Conferences")**







 
