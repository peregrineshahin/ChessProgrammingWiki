---
title: AVX512
---
**[Home](Home "Home") * [Hardware](Hardware "Hardware") * [x86-64](X86-64 "X86-64") * AVX-512**

**AVX-512**,

an expansion of [Intel's](Intel "Intel") [AVX](AVX "AVX") and [AVX2](AVX2 "AVX2") instructions using the [EVEX prefix](https://en.wikipedia.org/wiki/EVEX_prefix), featuring **32** 512-bit wide vector [SIMD](SIMD_and_SWAR_Techniques "SIMD and SWAR Techniques") registers zmm0 through zmm31, keeping either eight [doubles](Double "Double") or integer [quad words](Quad_Word "Quad Word") such as [bitboards](Bitboards "Bitboards"), and eight (seven) dedicated mask registers which specify which vector elements are operated on and written. If the Nth bit of a vector mask register is set, then the Nth element of the destination vector is overridden with the result of the operation; otherwise, dependent of the instruction, the element is zeroed, or overridden by an element from another source register (remains unchanged if same source). A vector mask register can be set using vector compare instructions, instructions to move contents from a GP register, or a special subset of vector mask arithmetic instructions.

## Contents

- [1 Extensions](#extensions)
- [2 Selected Instructions](#selected-instructions)
  - [2.1 VPTERNLOG](#vpternlog)
  - [2.2 VPLZCNT](#vplzcnt)
  - [2.3 VPOPCNT](#vpopcnt)
  - [2.4 VPDPBUSD](#vpdpbusd)
- [3 See also](#see-also)
- [4 SIMD](#simd)
- [5 Publications](#publications)
- [6 Manuals](#manuals)
- [7 Forum Posts](#forum-posts)
- [8 External Links](#external-links)
  - [8.1 Blogs](#blogs)
  - [8.2 Compiler Support](#compiler-support)
- [9 References](#references)

## Extensions

AVX-512 consists of multiple extensions not all meant to be supported by all AVX-512 capable processors. Only the core extension AVX-512F (AVX-512 Foundation) is required by all implementations <a id="cite-note-1" href="#cite-ref-1">[1]</a> AVX-512F and AVX-512CD were first implemented in the [Xeon Phi](https://en.wikipedia.org/wiki/Xeon_Phi) processor and coprocessor known by the code name [Knights Landing](https://en.wikipedia.org/wiki/Xeon_Phi#Knights_Landing) <a id="cite-note-2" href="#cite-ref-2">[2]</a> , launched on June 20, 2016.

|  Extension
|  Description
|  Architecture
| [CPUID 7](https://en.wikipedia.org/wiki/CPUID#EAX.3D7.2C_ECX.3D0:_Extended_Features)
Reg:Bit <a id="cite-note-3" href="#cite-ref-3">[3]</a>
|
| --- | --- | --- | --- |
|  AVX-512 F
|  Foundation
| [Knights Landing](https://en.wikipedia.org/wiki/Xeon_Phi#Knights_Landing) |  EBX:16
|
|  AVX-512 CD
|  Conflict Detection Instructions
|  EBX:28
|
|  AVX-512 ER
|  Exponential and Reciprocal Instructions
|  EBX:27
|
|  AVX-512 PF
|  Prefetch Instructions
|  EBX:26
|
|  AVX-512 BW
| [Byte](Byte "Byte") and [Word](Word "Word") Instructions
| [Skylake X](<https://en.wikipedia.org/wiki/Skylake_(microarchitecture)>) |  EBX:30
|
|  AVX-512 DQ
| [Doubleword](Double_Word "Double Word") and [Quadword](Quad_Word "Quad Word") Instructions
|  EBX:17
|
|  AVX-512 VL
|  Vector Length Extensions
|  EBX:31
|
|  AVX-512 IFMA
|  Integer Fused Multiply Add
| [Cannonlake](https://en.wikipedia.org/wiki/Cannonlake) |  EBX:21
|
|  AVX-512 VBMI
|  Vector Byte Manipulation Instructions
|  ECX:01
|
|  AVX-512 VPOPCNTDQ
|  Vector [Population Count](Population_Count "Population Count") | [Knights Mill](https://en.wikipedia.org/wiki/Xeon_Phi#Knights_Mill) |  ECX:14
|
|  AVX-512-4VNNIW
|  Vector [Neural Network](Neural_Networks "Neural Networks") InstructionsWord variable precision
|  EDX:02
|
|  AVX-512-4FMAPS
|  Fused Multiply AccumulationPacked Single precision
|  EDX:03
|
|  AVX-512 VNNI
|  Vector Neural Network Instructions Vector Instructions for [Deep Learning](Deep_Learning "Deep Learning") | [Ice Lake](<https://en.wikipedia.org/wiki/Ice_Lake_(microprocessor)>) |  ECX:11
|
|  AVX-512 VBMI2
|  Vector Byte Manipulation Instructions 2[Byte](Byte "Byte")/[Word](Word "Word") Load, Store and Concatenation with Shift
|  |
|  AVX-512 BITALG
|  Bit AlgorithmsByte/Word Bit Manipulation Instructions expanding VPOPCNTDQn
|  |
|  AVX-512 GFNI
|  Galois field New InstructionsVector Instructions for calculating [Galois Field](https://en.wikipedia.org/wiki/Finite_field) GF(2^8)
|  |

## Selected Instructions

## VPTERNLOG

AVX-512 F features the instruction VPTERNLOGQ (or VPTERNLOGD) to perform bitwise [ternary logic](https://en.wikipedia.org/wiki/Ternary_operation), for instance to [operate](General_Setwise_Operations "General Setwise Operations") on vectors of [bitboards](Bitboards "Bitboards"). Three input vectors are bitwise [combined](Combinatorial_Logic "Combinatorial Logic") by an operation determined by an immediate byte operand (**imm8**), whose 256 possible values corresponds with the boolean output vector of the [truth table](https://en.wikipedia.org/wiki/Truth_table) for all eight combinations of the three input bits, as demonstrated with some selected imm8 values in the table below <a id="cite-note-4" href="#cite-ref-4">[4]</a> <a id="cite-note-5" href="#cite-ref-5">[5]</a> :

|  Input
|  |  Output of Operations
|
| --- | --- | --- |
|  |  imm8
|  0x00
|  0x01
|  0x16
|  0x17
|  0x28
|  0x80
|  0x88
|  0x96
|  0xca
|  0xe8
|  0xfe
|  0xff
|
|  #
|  a
|  b
|  c
|  C-exp
|  false
|  ~(a|b|c)
|  a?~(b|c):b^c
|  minor(a,b,c)
|  c&(a^b)
|  a&b&c
|  b&c
|  a^b^c
|  a?b:c
| [major](General_Setwise_Operations#Majority "General Setwise Operations")(a,b,c)
|  a|b|c
|  true
|
|  0
|  0
|  0
|  0
|  |  0
|  1
|  0
|  1
|  0
|  0
|  0
|  0
|  0
|  0
|  0
|  1
|
|  1
|  0
|  0
|  1
|  |  0
|  0
|  1
|  1
|  0
|  0
|  0
|  1
|  1
|  0
|  1
|  1
|
|  2
|  0
|  1
|  0
|  |  0
|  0
|  1
|  1
|  0
|  0
|  0
|  1
|  0
|  0
|  1
|  1
|
|  3
|  0
|  1
|  1
|  |  0
|  0
|  0
|  0
|  1
|  0
|  1
|  0
|  1
|  1
|  1
|  1
|
|  4
|  1
|  0
|  0
|  |  0
|  0
|  1
|  1
|  0
|  0
|  0
|  1
|  0
|  0
|  1
|  1
|
|  5
|  1
|  0
|  1
|  |  0
|  0
|  0
|  0
|  1
|  0
|  0
|  0
|  0
|  1
|  1
|  1
|
|  6
|  1
|  1
|  0
|  |  0
|  0
|  0
|  0
|  0
|  0
|  0
|  0
|  1
|  1
|  1
|  1
|
|  7
|  1
|  1
|  1
|  |  0
|  0
|  0
|  0
|  0
|  1
|  1
|  1
|  1
|  1
|  1
|  1
|

Following VPTERNLOGQ intrinsics are declared, where the maskz version sets unmasked destination quad word elements to zero, while the mask version copies unmasked elements from s:

```

__m256i _mm256_ternarylogic_epi64(__m256i a, __m256i b, __m256i c, int imm8);
__m256i _mm256_maskz_ternarylogic_epi64(__mmask8 k, __m256i a, __m256i b, __m256i c, int imm8);
__m256i _mm256_mask_ternarylogic_epi64(__m256i src, __mmask8 k, __m256i a, __m256i b, int imm8);
__m512i _mm512_ternarylogic_epi64(__m512i a, __m512i b, __m512i c, int imm8);
__m512i _mm512_maskz_ternarylogic_epi64( __mmask8 m, __m512i a, __m512i b, __m512i c, int imm8);
__m512i _mm512_mask_ternarylogic_epi64(__m512i s, __mmask8 m, __m512i a, __m512i b, __m512i c, int imm8);

```

## VPLZCNT

AVX-512 CD has Vector [Leading Zero Count](BitScan#LeadingZeroCount "BitScan") - VPLZCNTQ counts leading zeroes on a vector of eight bitboards in parallel <a id="cite-note-6" href="#cite-ref-6">[6]</a> - using following intrinsics <a id="cite-note-7" href="#cite-ref-7">[7]</a>, where the maskz version sets unmasked destination elements to zero, while the mask version copies unmasked elements from s:

```

__m256i _mm256_lzcnt_epi64(__m256i a);
__m256i _mm256_maskz_lzcnt_epi64(__mmask8 k, __m256i a);
__m256i _mm256_mask_lzcnt_epi64(__m256i src, __mmask8 k, __m256i a);
__m512i _mm512_lzcnt_epi64(__m512i a);
__m512i _mm512_maskz_lzcnt_epi64(__mmask8 k, __m512i a);
__m512i _mm512_mask_lzcnt_epi64(__m512i src, __mmask8 k, __m512i a);

```

## VPOPCNT

The AVX-512 VPOPCNTDQ extension has a vector [population count](Population_Count "Population Count") instruction to count one bits of either 16 32-bit double words (VPOPCNTD) or 8 64-bit quad words aka bitboards (VPOPCNTQ) in parallel <a id="cite-note-8" href="#cite-ref-8">[8]</a> <a id="cite-note-9" href="#cite-ref-9">[9]</a> <a id="cite-note-10" href="#cite-ref-10">[10]</a>.

```

__m128i _mm_mask_popcnt_epi32(__m128i src, __mmask8 k, __m128i a);
__m128i _mm_maskz_popcnt_epi32(__mmask8 k, __m128i a);
__m128i _mm_popcnt_epi3 (__m128i a);
__m256i _mm256_mask_popcnt_epi32(__m256i src, __mmask8 k, __m256i a);
__m256i _mm256_maskz_popcnt_epi32(__mmask8 k, __m256i a);
__m256i _mm256_popcnt_epi32(__m256i a);
__m512i _mm512_mask_popcnt_epi32(__m512i src, __mmask16 k, __m512i a);
__m512i _mm512_maskz_popcnt_epi32(__mmask16 k, __m512i a);
__m512i _mm512_popcnt_epi32(__m512i a);

__m128i _mm_mask_popcnt_epi64(__m128i src, __mmask8 k, __m128i a);
__m128i _mm_maskz_popcnt_epi64(__mmask8 k, __m128i a);
__m128i _mm_popcnt_epi64(__m128i a);
__m256i _mm256_mask_popcnt_epi64(__m256i src, __mmask8 k, __m256i a);
__m256i _mm256_maskz_popcnt_epi64(__mmask8 k, __m256i a);
__m256i _mm256_popcnt_epi64(__m256i a);
__m512i _mm512_mask_popcnt_epi64(__m512i src, __mmask8 k, __m512i a);
__m512i _mm512_maskz_popcnt_epi64(__mmask8 k, __m512i a);
__m512i _mm512_popcnt_epi64(__m512i a)

```

## VPDPBUSD

The AVX-512 VNNI extension features several instructions speeding up [neural network](Neural_Networks "Neural Networks") and [deep learning](Deep_Learning "Deep Learning") calculations on the CPU, for instance [NNUE](NNUE "NNUE") inference using uint8/int8. VPDPBUSD - Multiply and Add Unsigned and Signed Bytes <a id="cite-note-11" href="#cite-ref-11">[11]</a>, executes on both port 0 and port 5 in one cycle <a id="cite-note-12" href="#cite-ref-12">[12]</a>.

```
 
__m512i _mm512_dpbusd_epi32 (__m512i src, __m512i a, __m512i b)
{
  for (j=0; j < 16; j++) {
    tmp1.word := Signed(ZeroExtend16(a.byte[4*j  ]) * SignExtend16(b.byte[4*j  ]);
    tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]);
    tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]);
    tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]);
    dst.dword[j] := src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4
  }
  return dst;
}

```

## See also

- [CFish - AVX2 Attacks](CFish#AVX2_Attacks "CFish")
- [DirGolem](DirGolem "DirGolem")
- [NNUE](NNUE "NNUE")
- [Stockfish NNUE](Stockfish_NNUE "Stockfish NNUE")
- [SIMD and SWAR Techniques](SIMD_and_SWAR_Techniques "SIMD and SWAR Techniques")

## SIMD

- [AltiVec](AltiVec "AltiVec")
- [AVX](AVX "AVX")
- [AVX2](AVX2 "AVX2")
- [SSE2](SSE2 "SSE2")
- [XOP](XOP "XOP")

## Publications

- [Mathias Gottschlag](https://os.itec.kit.edu/21_3247.php), [Frank Bellosa](https://os.itec.kit.edu/21_31.php) (**2018**). *[Mechanism to Mitigate AVX-Induced Frequency Reduction](https://os.itec.kit.edu/21_3486.php)*. [arXiv:1901.04982](https://arxiv.org/abs/1901.04982)
- [Mathias Gottschlag](https://os.itec.kit.edu/21_3247.php), [Philipp Machauer](https://os.itec.kit.edu/97_3742.php), [Yussuf Khalil](https://os.itec.kit.edu/21_3571.php), [Frank Bellosa](https://os.itec.kit.edu/21_31.php) (**2021**). *[Fair Scheduling for AVX2 and AVX-512 Workloads](https://www.usenix.org/conference/atc21/presentation/gottschlag)*. [USENIX ATC '21](https://www.usenix.org/conference/atc21)

## Manuals

- [Intel® Architecture Instruction Set Extensions Programming Reference](https://software.intel.com/sites/default/files/managed/c5/15/architecture-instruction-set-extensions-programming-reference.pdf) (pdf)
- [Intel® 64 and IA-32 Architectures Optimization Reference Manual](https://software.intel.com/content/www/us/en/develop/download/intel-64-and-ia-32-architectures-optimization-reference-manual.html)

## Forum Posts

- [AVX-512 and NNUE](http://www.talkchess.com/forum3/viewtopic.php?f=2&t=75049) by [Gian-Carlo Pascutto](Gian-Carlo_Pascutto "Gian-Carlo Pascutto"), [CCC](CCC "CCC"), September 08, 2020 » [NNUE](NNUE "NNUE")
- [VPOPCNTDQ and VBMI2](http://www.talkchess.com/forum3/viewtopic.php?f=7&t=77246) by [Vivien Clauzon](Vivien_Clauzon "Vivien Clauzon"), [CCC](CCC "CCC"), May 04, 2021

## External Links

- [AVX-512 from Wikipedia](https://en.wikipedia.org/wiki/AVX-512)
- [AVX-512 Vector Neural Network Instructions (VNNI) - x86 - WikiChip](https://en.wikichip.org/wiki/x86/avx512_vnni)
- [Intel® Advanced Vector Extensions 512 (Intel® AVX-512) Overview](https://www.intel.com/content/www/us/en/architecture-and-technology/avx-512-overview.html)
- [Intel Instruction Set Architecture Extensions](https://software.intel.com/en-us/intel-isa-extensions)

## Blogs

- [AVX-512 instructions | Intel® Developer Zone](https://software.intel.com/en-us/blogs/2013/avx-512-instructions) by [James Reinders](https://software.intel.com/en-us/user/335550), July 23, 2013
- [Future instruction set: AVX-512](http://www.agner.org/optimize/blog/read.php?i=288) by [Agner Fog](http://www.agner.org/), October, 09, 2013
- [Additional AVX-512 instructions | Intel® Developer Zone](https://software.intel.com/en-us/blogs/additional-avx-512-instructions) by [James Reinders](https://software.intel.com/en-us/user/335550), July 17, 2014
- [Processing Arrays of Bits with Intel® Advanced Vector Extensions 512 (Intel® AVX-512) | Intel® Developer Zone](https://software.intel.com/en-us/blogs/2014/07/24/processing-arrays-of-bits-with-intel-advanced-vector-extensions-512-intel-avx-512) by [Thomas Willhalm](https://software.intel.com/en-us/user/123920), July 24, 2014
- [AVX-512 May Be a Hidden Gem” in Intel Xeon Scalable Processors](https://www.hpcwire.com/2017/06/29/reinders-avx-512-may-hidden-gem-intel-xeon-scalable-processors/) by [James Reinders](https://software.intel.com/en-us/user/335550), [HPCwire](https://www.hpcwire.com/), June 29, 2017
- [Lower Numerical Precision Deep Learning Inference and Training](https://software.intel.com/content/www/us/en/develop/articles/lower-numerical-precision-deep-learning-inference-and-training.html) by [Andres Rodriguez](https://community.intel.com/t5/user/viewprofilepage/user-id/134067) et al., January 19, 2018

## Compiler Support

- [Intel Intrinsics Guide - AVX-512](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#techs=AVX_512)
- [Intel® Advanced Vector Extensions 2015/2016 Support in GNU Compiler Collection](https://gcc.gnu.org/wiki/cauldron2014?action=AttachFile&do=get&target=Cauldron14_AVX-512_Vector_ISA_Kirill_Yukhin_20140711.pdf) (pdf) by [Kirill Yukhin](https://www.linkedin.com/in/kirill-yukhin-1158374/), July 2014
- [Guide to Automatic Vectorization with Intel AVX-512 Instructions in Knights Landing Processors - Colfax Research](https://colfaxresearch.com/knl-avx512/), May 11, 2016
- [Microsoft Visual Studio 2017 Supports Intel® AVX-512 | Visual C++ Team Blog](https://blogs.msdn.microsoft.com/vcblog/2017/07/11/microsoft-visual-studio-2017-supports-intel-avx-512/) by Eric Battalio, July 11, 2017

## References

1. <a id="cite-ref-1" href="#cite-note-1">[1]</a> [AVX-512 from Wikipedia](https://en.wikipedia.org/wiki/AVX-512)
1. <a id="cite-ref-2" href="#cite-note-2">[2]</a> [Additional AVX-512 instructions | Intel® Developer Zone](https://software.intel.com/en-us/blogs/additional-avx-512-instructions) by [James Reinders](https://software.intel.com/en-us/user/335550), July 17, 2014
1. <a id="cite-ref-3" href="#cite-note-3">[3]</a> [AVX512 table](https://www.heise.de/ct/zcontent/17/16-hocmsmeta/1501873687265857/ct.1617.016-017.qxp_table_29578.html) from [Heise](https://en.wikipedia.org/wiki/Heinz_Heise)
1. <a id="cite-ref-4" href="#cite-note-4">[4]</a> [AVX512: ternary functions evaluation](http://0x80.pl/articles/avx512-ternary-functions.html) by [Wojciech Muła](Wojciech_Mu%C5%82a "Wojciech Muła"), March 03, 2015
1. <a id="cite-ref-5" href="#cite-note-5">[5]</a> [Intel® Architecture Instruction Set Extensions Programming Reference](https://software.intel.com/sites/default/files/managed/c5/15/architecture-instruction-set-extensions-programming-reference.pdf) (pdf) 5.3 TERNARY BIT VECTOR LOGIC TABLE
1. <a id="cite-ref-6" href="#cite-note-6">[6]</a> [Patent US9372692 - Methods, apparatus, instructions, and logic to provide permute controls with leading zero count functionality - Google Patent Search](https://www.google.com/patents/US9372692)
1. <a id="cite-ref-7" href="#cite-note-7">[7]</a> [VPLZCNTD/Q—Count the Number of Leading Zero Bits for Packed Dword, Packed Qword Values](https://hjlebbink.github.io/x86doc/html/VPLZCNTD_Q.html)
1. <a id="cite-ref-8" href="#cite-note-8">[8]</a> [sse-popcount/popcnt-avx512-harley-seal.cpp at master · WojciechMula/sse-popcount · GitHub](https://github.com/WojciechMula/sse-popcount/blob/master/popcnt-avx512-harley-seal.cpp)
1. <a id="cite-ref-9" href="#cite-note-9">[9]</a> [Wojciech Muła](Wojciech_Mu%C5%82a "Wojciech Muła"), [Nathan Kurz](http://dblp.uni-trier.de/pers/hd/k/Kurz:Nathan), [Daniel Lemire](https://github.com/lemire) (**2016**). *Faster Population Counts Using AVX2 Instructions*. [arXiv:1611.07612](https://arxiv.org/abs/1611.07612)
1. <a id="cite-ref-10" href="#cite-note-10">[10]</a> [Intel® Intrinsics Guide VPOPCNTD](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=VPOPCNTD&expand=4368)
1. <a id="cite-ref-11" href="#cite-note-11">[11]</a> [Intel® Intrinsics Guide VPDPBUSD](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#expand=2168,2201&text=VPDPBUSD&avx512techs=AVX512_VNNI)
1. <a id="cite-ref-12" href="#cite-note-12">[12]</a> [Lower Numerical Precision Deep Learning Inference and Training](https://software.intel.com/content/www/us/en/develop/articles/lower-numerical-precision-deep-learning-inference-and-training.html) by [Andres Rodriguez](https://community.intel.com/t5/user/viewprofilepage/user-id/134067) et al., January 19, 2018

**[Up one Level](X86-64 "X86-64")**

